\chapter{System Design}
\thispagestyle{plain}
\label{System Design}


\section{Architecture Overview}

The architecture of the system comprises of several modules that process a given input log file, and an RDF (Resource Description Framework) file describing columns and relations is given as the output. The modular nature of the architecture allows us to add more specialists to the existing framework. Figure~\ref{fig:system_architecture} shows the block diagram of the system architecture. It begins with \textit{Tabulate} module, where the input log file is parsed to detect its structure. Once the structure is detected the structured log file is separated into columns. These columns are then passed to the \textit{Decode} stage. In the \textit{Decode} module the columns are checked against various classifiers to find a match. After checking against different classifiers, the columns are given a score against every classifier in the system. The scores form a ranked list of probable classes for the columns in the log file. This ranked list is used in the \textit{Relationship Generator} to identify the relations between various columns in the log file. Once the columns are identified and the relationships have been detected, the \textit{Generate RDF} module will produce a set of RDF triples to represent the inferred semantics. The four major modules have been described in detail in rest of the section.


\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth, height=0.5\textheight, keepaspectratio] {system_architecture.png}
	\caption{System Architecture}
	\label{fig:system_architecture}
\end{figure}


\subsection{Tabulate}
\label{Tabulate}

The \textit{Tabulate} module takes the structured log file as input and gives out a table with possible set of columns to the \textit{Decode} module for classification. The input log file can have any structure and the module will try to detect columns in it. The log file undergoes multiple iterations for the following reasons:
\begin{itemize}
\item To split the log file based on delimiters
\item To detect consistent number of columns throughout the log file
\item To detect and separate sub-columns
\end{itemize}

In the first iteration of the \textit{Tabulate} module, we split the log files using generic delimiters like space, square braces, round braces, single and double quotes. For braces and quotes we consider only those that are in pairs. Every row is thus separated in varied number of columns that may or may not be consistent. 

Although most of the lines in a structured log file have fixed number of columns, it is not necessary that after separating on the above delimiters we will get a table with same number of columns. Also, some log entries can be outliers due to consistency in the way the logs are generated. But in most of the cases, there is a textual description at the end of the log entry which does not have a fixed number of words. If the description is not enclosed in a brace or quote, then it will be divided into several different columns. Considering this fact, we dynamically decide the number of columns that are to be extracted. For this, we put a threshold on the percentage of rows that will be considered in the system. We select rows starting with the ones that have the highest number of columns. We then select the next set of rows that have the second highest number of columns. We continue this process till we reach the threshold number of rows. At this moment we record the number of columns of the current set of rows. This is the number of columns that we choose to output from the system. All the rows having columns less than the selected number of columns are discarded. We keep the threshold pretty high (above 70 \%), so as to collect more number of rows. We also tried keeping the rows that have columns less than the selected number of columns and utilize them in the \textit{Decode} module. It was observed if a large percent of log entries have consistent columns, then the remaining smaller percent usually forms abnormal entries. Thus they create noise in the \textit{Decode} module when trying to detect the class of the column.

In the third iteration, we loop through the columns that are already separated in the previous iterations. In this iteration, we check the columns to detect sub-columns. We use the same set of delimiters to split the columns further. If all the elements in a particular columns are separable, then we split the column into multiple sub-columns. Before separation, we run that complete column through the \textit{Decode} module to check if the whole column matches any of the known classes. If there is a match found, then there is no need of splitting the column, as we can use the column as a whole.


\subsection{Decode}
\label{Decode}

The \textit{Decode} module does the work of assigning classes to the columns given by the \textit{Tabulate} module. For the classification of columns we created an extension of the specialists approach implemented by Puranik~\cite{puranik2012specialist}. Puranik defines a specialist as ``an entity that has expertise in the domain for which it is a specialist''. The mentioned specialists system was created for generic table data found on the Internet like SSN, phone numbers, addresses, etc. We extend this system to classify specific fields that are usually found in log files viz., timestamps, IP addresses, URLs, etc.

Every column is passed through the specialists in the system. All the specialists give their score to the column based on how well the column matches the class of the specialist. We normalize this score to form a ranked list to predict the class of the system. If there is no match to any of the specialists then the columns is classified as `NA' (No Annotation). The normalized scores can also be considered as the confidence with which the specialists classifies a particular column into its respective class.

Specialists can be of different types like regular expression based specialists, dictionary based specialists and classifier based specialists. Currently we are using only regular expression based and dictionary based specialists. But we can further extend it to use classifier based specialists as well. Following is the list of specialists present in the current system:

\begin{enumerate}
\item \textbf{Timestamp Specialist}\\
The Timestamp specialist is a regular expression based specialist, which handles a lot of timestamp formats which are used in various systems for logging. This could also be implemented as a classifier based specialist using the Date specialist implemented by Puranik~\cite{puranik2012specialist}.
\item \textbf{IP Address Specialist}\\
The IP Address specialist is a regular expression based specialist. It checks for valid IP address formats and also makes sure that they are in a valid range.
\item \textbf{Port Number Specialist}\\
The Port number specialist is a regular expression based specialist. It checks for valid port numbers, that is, those that are in the range of 0 to 65535.
\item \textbf{URL Specialist}\\
The URL specialist is a regular expression based specialist, which looks for various URL formats viz., \textit{HTTP}, \textit{HTTPS}, \textit{FTP}, \textit{FTPS}. It also detects URL having basic authentication.
\item \textbf{Filepath Specialist}\\
The Filepath specialist is also a regular expression based specialist. It looks for filepaths irrespective of the underlying system.
\item \textbf{Email Specialist}\\
The Email specialist is a regular expression based specialist, that looks for valid email formats in the given column. This follows the Internet standards to detect a valid email.
\item \textbf{HTTP Status Code Specialist}\\
The HTTP Status Code specialist is dictionary based specialist. It looks for the HTTP status code values provided by the W3 standards. These HTTP status codes are integer values.
\item \textbf{HTTP Method Specialist}\\
The HTTP Method specialist is a dictionary based specialist. It looks for the known HTTP verbs / methods like \textit{GET}, \textit{PUT}, \textit{POST}, \textit{DELETE}, etc.
\item \textbf{HTTP Protocol Version Specialist}\\
The HTTP Protocol Version specialist is a dictionary based specialist, which looks for the known HTTP protocol versions. The HTTP protocol versions are in a string format.
\item \textbf{Log Level Specialist}\\
The Log level specialist is a dictionary based specialist, which looks for the generally recommended log levels keywords. The log levels are in string format and contains log levels like \textit{Info}, \textit{Debug}, \textit{Error}, \textit{Warn}, etc.
\end{enumerate}

We can easily extend the system to add more specialist to detect other fields in the log files. These specialists can be regular expression based or dictionary based specialists. For instance, if a system administrator decides to have a specialist for server names, then they can have a dictionary based specialist. They will keep a dictionary with all the server names. In future, if  our system comes across a log file with server name as the column, it will detect and classify the column precisely.
 

\subsection{Relationship Generator}
\label{Relationship Generator}

The \textit{Decode} module gives out a ranked list of class labels for every column in the table, which is further used to predict the relations between various pairs of columns. We consider the topmost ranked prediction as the class label for that particular column. Considering a pair of columns at a time, we search the \textit{Knowledge Base} to find relationship between them. Every column is checked against all other predicted columns in the table. For now, we do not consider the columns that are not annotated by the \textit{Decode} module.

In the current experiments we have used the IDS Ontology\footnote{http://ebiquity.umbc.edu/ontologies/cybersecurity/ids/v2.3/IDSOntology.owl} as our knowledge base. This OWL ontology which was created at University of Maryland, Baltimore County, was developed for Intrusions Detection Systems. We extended this ontology to add certain classes and relations that were found in log files that we collected.

We use RDFLib, a python library to parse the ontology so that it becomes easy to search it. Using the SPARQL plugin in the RDFLib, we search for classes that act as domain and/or range for properties. For instance, when we are given a pair of columns, we find all the properties where one class is domain and the other is range or vice versa. This way we extensively search for all combinations of columns in the ontology.

We give multiple relationships if there are multiple finds for a particular pair of columns. In case of log files, we do not have the instances of columns in knowledge base. If there were instances present, then using an approach similar to that developed by Mulwad~\cite{mulwad2015tabel}, we could have ranked the relations.  


\subsection{Generate RDF}
\label{Generate RDF}

The \textit{Relationship Generator} module gives a list of triples: subject, object and the relation. We serialize these triples using the RDFLib library to create an RDF file. This RDF file with triples can be used by systems like SIEMs or Intrusion Detection Systems to feed into their system. Using this information these systems will now know the possible columns and relations in any given log file and will not need human intervention to interpret the log files.
