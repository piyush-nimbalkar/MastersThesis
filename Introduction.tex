\chapter{INTRODUCTION}
\thispagestyle{plain}

\label{Introduction}

The advancement of software has led to creation of huge amount of data. One such kind of data that is generated behind almost every system today, is logs. Every single thing we do on a software system results in triggering various pieces of software, and in turn creating several lines of logs. Log data is no longer just a tool for diagnostics and debugging software. It is increasingly used in cybersecurity, forensics, and for auditing purposes, due to the amount of vital information it holds. Recognizing the importance of logs, enterprises have started spending a lot of money to generate and store huge amount of log daily. Also, special emphasis is given to ensure creation of auditing information, so that it becomes easy to track user activities.

Log files give vital information about systems and the activities that are occurring. As such they present a valuable insight into the state of the system. Looking at the log file, not only do we understand the event, but can also trace its origin by audit trails and forensics. For instance, database logs can help trace the modifications on data, while web server log files speak a lot about the resources accessed from the web. On one hand, operating system logs help us figure out what is happening at the system level, on the other, firewall logs help record malicious activities at the network level. Thus, log files form a vital tool in the cybersecurity. Using the information from log files we can pro-actively defend our systems against potential malicious activities and attacks.

Log file analysis is useful not only in defensive roles, but can also prove useful in the offensive approach to cybersecurity. Although offensive security techniques have ethical, legal, technical and practical considerations, it can be certainly useful in mitigating cyber threats~\cite{offensive_cybersecurity}. There can be intrusive malware to track and observe the activities of targeted suspicious actors. This malware could be an intelligent log analyzer, trying to decode information from logs collected via various sources. Such a knowledge can help us secure our systems against zero day attacks.


\section{Motivation}

Huge amount of data generated in the form of logs is descriptive in nature because that was the primary purpose of logs. Now-a-days, people have started giving emphasis on structured log data. This eases the understanding and management of the logs. Today, a large amount of log files have a structure, which include the textual descriptions at the end. The structured log files also convey more information with less verbosity. Log formats are also being standardized, so that every vendor follows a format for certain set of tools or softwares. For instance, such recommendations provided in ``Guide to Computer Security Log Management'', published by the National Institute of Standards and Technology (NIST)~\cite{nist_guide}, indicate the growing importance of standardizing and analyzing log files.

It is important to know the structure of the log files to extract more and precise information. The difficulties in finding the exact structure of a log file are described by Kimball and Merz~\cite{kimball_log_problem}. They include multiple file formats, incomplete, inconsistent and irrelevant data, and dispersed information. There are even log files which have multi-line log entries.

There have been various attempts to tackle these problems with log files~\cite{log_parser_2005}. There are tools that parse specific log files and extract information from them~\cite{weblog_expert}. However, this is not a scalable approach as it requires us to know the file format of a log file before analyzing it. Also, there are text processing techniques which extract information from log files as they would do from a normal chunk of text~\cite{saneifar2009terminology}. This information is used by Intrusion Detection and Prevention systems to detect malicious behavior. However, if we can leverage the structure of log files, we can clearly understand events in the log files and get more information from them. Most log files have distinct columns that do not seem to be related to each other and have a similar set of values. For instance, in \textit{apache\_access} log file, we have \textit{content length} which is a numeric column and a \textit{request path} which are file paths. If we know the meaning of the columns in the log file, we can easily say that the request on the \textit{request path} gave a response of size equal to the \textit{content length}. Often, there are semantic relations between various columns of log files. These may be documented somewhere but it is difficult for an autonomous system to understand the relations as a human would.

Apart from text processing systems, there are enterprise tools like Splunk \footnote{http://www.splunk.com/}, Sumologic \footnote{https://www.sumologic.com/}, etc. These tools primarily focus on log management and analytics. Splunk does detect various fields in the log file but it does not always separate it out in proper columns. This happens particularly when it does not know the structure or source of the log file before hand. Thus, such enterprise tools have serious limitations in predicting the structure of unknown log files and further do not deal with finding semantic connections between various columns of the log file.


\section{Contribution}

To solve these problems we have built a framework that takes a log file as input and gives out its semantic interpretation as Linked Open Data expressed in RDF (Resource Data Framework)~\cite{brickley2004rdf}. Our framework works for any random log file that has structured columns. We split the log file into identifiable columns and then predict classes for them. Using these columns we generate a list of candidate relations between those columns whose classes we have predicted, which are given out into a RDF file. We have extended the IDSOntology \footnote{http://ebiquity.umbc.edu/ontologies/cybersecurity/ids/v2.3/IDSOntology.owl} to help identify the relationships between various column classes. Moreover, other ontologies can also be used. We test the system against log files with different structure and unknown sources. We also test the system against a dataset of randomly generated synthetic log files.
