\chapter{Overview}
\thispagestyle{plain}
\label{Overview}

The framework we propose works well with structured log files, where we can separate log files into distinct columns and then predict the possible relations between those. We propose a scalable column detection module wherein we can plug in more column expert modules to identify other columns. Currently we support ten distinct classes of columns.

The part of predicting the semantic relations between various columns in the log file is our novel contribution to the world of log file analysis. Finding relations definitely benefits in understanding the semantics of the log files. Having internal relations can help make the log files less verbose, by preventing the need to explain every detail. Logs are written such that they do not become extremely verbose. As the size increases, the cost to maintain the huge amount of logs increases. That is why we find log files with concise columns. We see columns like IP addresses, timestamps, email addresses, urls etc. Most of the columns in the log files have inherent connections to each other. These are not visible by looking at the log file and are documented somewhere else. For example let us consider, \textit{sendmail} log file, which is a tool for email routing. Looking at the log file we see random server names and email addresses. If a automated system can figure out the relationship between these fields they we can that the email is sent or received from the server name mentioned in the log entry.

Predicting the structure and the semantic meaning for a log file with unknown source or format definitely possess an advantage in cybersecurity log analysis. Every year, large number of tools and devices added to the current software world. A security specialist or system administrator would not want to manually analyze every log file and then feed it into their system. 

The framework is also crucial in Security Information and Event Management (SIEM) systems, to detect anomalous behavior in the system. If the SIEM, is able to know the structure of any random log file in the infrastructure, it can autonomously detect threats. We can defend out systems against unknown attacks before hand. For instance, an apache access log files has columns like IP address, resource path and user-agent out of the many. The IP address is the address of the machine requesting the resource accessed by the resource path. User-agent is the software which can request the resource for the user of the IP address. This can be anything from a browser to command line tool. Knowing about the log file, now we can have relations like `Resource\_Path \textit{requestedUsing} User-agent' or `IP\_Address \textit{requestedResource} Resource\_Path'. With this knowledge SIEMs can detect threats like IP addresses accessing unauthorized resources or if they are accessing using suspicious user-agents. This becomes easy on knowing the semantic information about the log file.

The world of Semantic Web has huge amount of data and is still growing. There are knowledge bases or ontologies which already have many classes and properties that are found in the columns of log files. If this data is extended to support different kinds of data found in log files, we will have a huge collection of sematic log file data. This can be used to process and smartly analyze generic log files.

The details of how the framework is implemented will be discussed in the $4^{th}$ chapter. Before that we will briefly introduce some concepts and terminologies used in this thesis for better understanding.

\section{Linked Data}
